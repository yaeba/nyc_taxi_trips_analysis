---
title: "New York City Taxi Trips Analysis"
author: "Xuanken Tay"
date: "September 2, 2019"
output: 
  html_document:
    toc: true
    toc_float: true
    number_sections: true
    fig_width: 6
    fig_height: 3.5
    fig_caption: true
    theme: readable
    highlights: tango
    code_folding: hide
---



```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE, 
                      fig.align = "center")
```



# Introduction

The aim of this project is to make a qualitative analysis and gain insights into the open source New York City Taxi and Limousine Service Trip Record Data. In this notebook, we will tackle a reserach problem and perform analysis on millions of **taxi trips data** --- all in a [reproducible](https://en.wikipedia.org/wiki/Reproducibility#Reproducible_research) manner with R, in that the results are accompanied by the data and code needed to produce them.<sup>[1]</sup>  

Source code of this Rmarkdown can be found in the GitHub [repo](https://github.com/yaeba/nyc_taxi_trips_analysis) if one wishes to reproduce the exact same results of this study. Likewise, an initial exploratory analysis of the data can be viewed [here](https://yaeba.github.io/nyc_taxi_trips_analysis/nyc_taxi_trips_analysis_visualization.html).  


***


# Research Problem

Despite predicting taxi fare is not as rewarding as [predicting airline fares](https://www.business-standard.com/article/international/predicting-airfare-is-a-750-million-business-118100301166_1.html)<sup>[2]</sup>, we may want to start practicing from predicting the former given that now we have an extensive amount of open source data available.   

In this study, we are interested in understanding **how taxis arrive at fares** to be paid by the passengers. To do so, we will first access and report the relation between the taxi fares (*response*) with other attributes (*explanatory*). Following that, we will build statistical models in order to predict taxi fares for rides in the New York City given the input variables. The final model could allow passenger to make use of this knowledge before calling for a taxi service.  


## Hypothesis

To approach this research problem, we will first list down a set of **hypotheses**, which in our case are potential factors that could affect the cost of a taxi trip.  

1.  **Trip distance**: Taxi fare is higher for long rides.
2.  **Trip duration**: Trips caught in traffic jam may have higher fares.
3.  **Pickup or dropoff location**: Fares may be different depending on the location travelling to/from.
4.  **Time of travel**: Fare amount may differ during peak and non-peak hours.
5.  **Day of travel**: On weekends, taxi fares may be higher.
6.  **Trip to/from airport**: Rides to airport have a [flat fare](https://www.airport-jfk.com/taxi.php)<sup>[3]</sub>.
7.  **Weather conditions**: Bad weather condition may affect the availability and thus results in higher taxi fare.


For the purpose of this study, we will use the following R packages:  

```{r libraries}
library(tidyverse)
library(data.table)
library(rgdal)
library(lubridate)
library(geosphere)
library(corrplot)
library(h2o)
```

- [tidyverse](https://www.tidyverse.org/): to help in general manipulating the dataframes and plotting  
- [data.table](https://github.com/Rdatatable/data.table): to enable efficient reading and pre-processing of datasets  
- [rgdal](https://github.com/cran/rgdal): R’s interface to the popular C/C++ spatial data processing library `gdal`  
- [lubridate](https://github.com/tidyverse/lubridate): allows to manipulate datetime variables  
- [geosphare](https://www.rdocumentation.org/packages/geosphere): spherical trigonometry for geographic applications  
- [corrplot](https://cran.r-project.org/web/packages/corrplot/corrplot.pdf): package for graphical display of a correlation matrix  
- [h2o](http://h2o-release.s3.amazonaws.com/h2o/master/3552/docs-website/h2o-docs/SiteIntro.html): R interface to distributed in-memory machine learning platform
  
  

***



# Data Preparation

In any analytics project, 80% of the time and effort is spent on preparing, cleaning and organizing data for analysis<sup>[4](https://www.forbes.com/sites/gilpress/2016/03/23/data-preparation-most-time-consuming-least-enjoyable-data-science-task-survey-says/#18b4b12f6f63)</sup>. This section describes briefly the data used and the steps taken to process and prepare them for the study.  

## Data Loading

We will use the following open souce dataset in the analysis.

### New York City Taxi & Limousine Commission (TLC) Service Trip Record Data

The primary dataset we will be using in this study. The dataset covers trips taken in various different types of licensed taxi and limousine services in the New York City area from 2009 to 2019. Briefly, each taxi trip records pick-up and drop-off datetime and location, trip distance, itemized fare, payment type, and driver-reported passenger count.  


### TLC Taxi Zone Data

New York City taxi zone locations information from TLC that includes:  

-  a lookup table that contains TLC taxi zone location IDs, location names and corresponding boroughs for each ID.  
-  a polygon shapefile containing the boundaries for the TLC taxi zones.  


```{r}
# read in taxi zones information
zones_lookup <- read_csv("data/taxi+_zone_lookup.csv", col_types="iccc")
taxi_zones <- readOGR(dsn = "data/taxi_zones/taxi_zones.shp", verbose=FALSE)
taxi_zones$id <- row.names(taxi_zones)

# merge zone information with taxi service zone in lookup table
taxi_zones@data <- taxi_zones@data %>%
  left_join(zones_lookup %>% 
              select(-LocationID, -Borough) %>%
              unique(), 
            by=c('zone' = 'Zone')) %>%
  mutate(zone = as.factor(zone),
         service_zone = as.factor(service_zone))

# reproject to commonly used CRS
taxi_zones <- spTransform(taxi_zones, CRS("+init=epsg:4326"))
```

### Holidays

Dataset primarily created in python to support the taxi fare analysis. It contains list of public holidays for New York at the state level for the year 2014 and 2015.  

```{r}
#############################################
## python code to generate ny_holidays.csv ##
# from datetime import date
# import pandas as pd
# import holidays
# 
# ny_holidays = holidays.US(state='NY', years=[2014, 2015])
# 
# df = pd.DataFrame(list(ny_holidays.items()), columns=["Date", "Holiday"])
# 
# df.to_csv("ny_holidays.csv", index=False)
############ end of python code #############

holidays <- read.csv("data/ny_holidays.csv") %>%
  mutate(Date = ymd(Date))
```

### New York City's Central Park Weather Data

In order to study how weather conditions affect taxi fare (*hypothesis 7*) in New York City, we will use the New York City Central Park weather data that comes from [National Climatic Data Center](https://www.ncdc.noaa.gov/) which contains daily weather observation at the central park for year 2014 and 2015.  
One limitation of using this dataset is that we will need to assume that entire New York City will share the exact same weather condition with central park. Moreover, aggreagted daily data may be crude as weather could fluctuate from hour to hour.  

```{r}
weather <- read.csv("data/ny_city_central_park_weather.csv") %>%
  mutate(DATE = ymd(DATE),
         TAVG = (TMIN + TMAX) / 2,
         AWND = ifelse(is.na(AWND), mean(AWND, na.rm=TRUE), AWND),
         AWND = round(AWND, digits=2)) %>%
  select(-NAME, -TMAX, -TMIN)
```


## Data Selection

We will look at green taxi trips for this study. Green taxis (as opposed to yellow ones) are taxis that are not allowed to pick up passengers inside of the densely populated areas of Manhattan. They do not seem to attract much attention of the research community in comparison with the relatively larger yellow taxi but may also contains interesting human behaviour-related information.  


### Period Selection

The entire green taxi dataset is large, covering from year 2013 to 2019. For the purpose of this study, we will look at only the data from year 2014 to year 2015 as it contains precise pickup and dropoff location information in latitude and longitude - which may be helpful to in deriving the neighbourhoods and identifying airport trips (as needed for *hypothesis 3 and 6*). Furthermore, spanning the scope of the analysis across whole year allows us take in account of the effect of seasonality.  

### Attributes Selection

With all the data as above, we will look at a number of attributes for the study. Specifically, the following attributes will be used to aid in our analysis:  

[`Total Fare`, `Pickup and Dropoff Location`, `Pickup and Dropoff Day and Time`, `Trip Distance and Duration`, `Weather Condition` and `Holiday`]  

The selection of these attributes are well justified by *hypothesis 1 to 7*, in that we need them to in order to try to infer the result of hypotheses. For the purpose of this study, we will regard `Total Fare` as the amount required to be paid by passenger excluding the tips.  


```{r}
compute_total_fare <- function(taxi_data) {
  taxi_data %>%
    mutate(Total_fare = Total_amount - Tip_amount)
}

compute_trip_duration <- function(taxi_data) {
  taxi_data %>%
    rename(Pickup_datetime = lpep_pickup_datetime,
           Dropoff_datetime = Lpep_dropoff_datetime) %>%
    mutate(Pickup_datetime = ymd_hms(Pickup_datetime),
           Dropoff_datetime = ymd_hms(Dropoff_datetime),
           Trip_duration = difftime(Dropoff_datetime, Pickup_datetime, units="mins"),
           Trip_duration = as.numeric(Trip_duration) %>% round(digits=2))
}
```


## Data Cleansing

It is important to clean the data as removing incorrect information can improve the data quality and in doing so, increases overal productivity. The trips data has been checked and none of rows - with chosen attributes contains missing value. Next the data was checked to remove any abnormal or wrong records. This step is important as it ensures the data is correct, consistent and useable by identifying any errors or inaccurate information in the data. Briefly, bad records or outliers have been filtered out according to the follwing criteria:  

-   Number of passengers in the vehicle $= 0$ or $> 6$: There is a maximum passenger limit imposed on any cabs<sup>[5](https://www.tripsavvy.com/new-york-city-taxis-4026457)</sup>.  
-   Pickup/Dropoff latitude $\notin (39, 42)$ or Pickup/Dropoff longitutde $\notin (-76, -72)$: Only trips within New York<sup>[6](https://en.wikipedia.org/wiki/New_York_City)</sup> are of interest.  
-   Total fare $<= 0$ or $> 300$ dollars: Remove negative values and fare above 0.999 quantile (outliers).    
-   Trip duration $< 1$ minute or $> 12$ hours: Remove trips that are too short or too long (outliers).



```{r}
clean_taxi_data <- function(taxi_data) {
  taxi_data %>%
    # remove rows with missing value(s)
    drop_na() %>%
    # data cleansing
    filter(Passenger_count > 0, Passenger_count <= 6,
           Pickup_latitude > 39, Pickup_latitude < 42,
           Pickup_longitude > -76, Pickup_longitude < -72,
           Dropoff_latitude > 39, Dropoff_latitude < 42,
           Dropoff_longitude > -76, Dropoff_longitude < -72,
           Total_fare > 0, Total_fare < 300,
           Trip_duration > 1, Trip_duration < (12 * 60))
}
```


## Data Preprocessing

Data preprocessing has been carried out in order to prepare the data for our analysis. Very briefly, the steps involved in transforming the data into a format useful for our study are:  

1. All the taxi trips’ pickup and dropoff locations were overlaid on spatial polygons defining every taxi zone to find pickup and dropoff location associated with each trip. This information is useful for downstream spatial analysis of taxi fare with each taxi zone.  

```{r}
spatial_overlap_zones <- function(taxi_data, zones=taxi_zones) {
  # find intersect between trip locations and taxi zones
  pickup_zones <- SpatialPointsDataFrame(coords=taxi_data[, c('Pickup_longitude', 'Pickup_latitude')],
                                         data=taxi_data,
                                         proj4string=CRS(proj4string(zones))) %>%
    over(zones) %>%
    pull(zone)
  
  dropoff_zones <- SpatialPointsDataFrame(coords=taxi_data[, c('Dropoff_longitude', 'Dropoff_latitude')],
                                          data=taxi_data,
                                          proj4string=CRS(proj4string(zones))) %>%
    over(zones) %>%
    pull(zone)
  
  taxi_data %>%
    mutate(Pickup_zone = factor(pickup_zones, levels=levels(zones$zone)),
           Dropoff_zone = factor(dropoff_zones, levels=levels(zones$zone))) %>%
    # some trips may not fall in known taxi zones
    drop_na()
}
```

2. Identify airport rides as trips that either  
  + start or terminate at the 3 NYC area airports: John F. Kennedy International Airport (JFK), LaGuardia Airport (LGA), and the Newark Liberty International Airport (EWR) or  
  + have `RateCodeID` of 2 (JFK) or 3 (EWR)  
  
```{r}
# find airport zones
airport_zones <- taxi_zones@data %>%
  filter(service_zone %in% c("Airports", "EWR")) %>%
  pull(zone) %>%
  as.vector()


identify_airport_trips <- function(taxi_data, zones=airport_zones) {
  taxi_data %>%
    mutate(Airport_trip = Pickup_zone %in% zones |
                          Pickup_zone %in% zones |
                          Dropoff_zone %in% zones |
                          Dropoff_zone %in% zones |
                          RateCodeID %in% c(2, 3))
}

```

3. Extract month, day of week and hour from trips and then determine the season<sup>[6](https://www.timeanddate.com/calendar/aboutseasons.html)</sup>, weekday/weekend and time of day for all trips.  

```{r}
season <- function(datetime) {
  ## ref https://www.timeanddate.com/calendar/aboutseasons.html
  m <- month(datetime)
  terms <- c("Spring", "Summer", "Autumn", "Winter")
  s <- factor(character(length(m)), levels=terms)
  s[m %in% c(3, 4, 5)] <- terms[1]
  s[m %in% c(6, 7, 8)] <- terms[2]
  s[m %in% c(9, 10, 11)] <- terms[3]
  s[m %in% c(12, 1, 2)] <- terms[4]
  s
}


extract_time_features <- function(taxi_data) {
  taxi_data %>%
    mutate(Pickup_month = month(Pickup_datetime, label=TRUE),
           Pickup_day = wday(Pickup_datetime, label=TRUE),
           Pickup_hour = hour(Pickup_datetime),
           Season = season(Pickup_datetime),
           Weekend = Pickup_day %in% c("Sat", "Sun"),
           Time = ifelse(Pickup_hour >= 6 & Pickup_hour < 18,
                         "Daytime",
                         "Nighttime"),
           Time = factor(Time, levels=c("Daytime", "Nighttime")))
}

```

4. Compute orthodromic distance -- shortest distance between two points on the surface of a sphere<sup>[7](https://en.wikipedia.org/wiki/Great-circle_distance)</sup>.  

```{r}
great_circle_distance <- function(taxi_data) {
  taxi_data %>%
    mutate(Great_circle_dist = distHaversine(
      cbind(Dropoff_longitude, Dropoff_latitude),
      cbind(Pickup_longitude, Pickup_latitude)) %>%
        round(digits=2))
}

```

5. Combine with external data in order to gain information on weather conditions and if the day is holiday or not.  

```{r}
join_external_data <- function(taxi_data, holiday_data=holidays, weather_data=weather) {
  taxi_data %>%
    # determine if the day is holiday
    mutate(Pickup_date = as.Date(Pickup_datetime),
           Holiday = Pickup_date %in% holiday_data$Date) %>%
    # join with weather data
    left_join(weather_data, by=c("Pickup_date" = "DATE")) %>%
    select(-Pickup_date)
}

preprocess_taxi_data <- function(taxi_data) {
  taxi_data %>%
    spatial_overlap_zones() %>%
    identify_airport_trips() %>%
    extract_time_features() %>%
    great_circle_distance() %>%
    join_external_data()
}
```


## Data Processing

With all the functions needed for cleaning and pre-processing the data, we can now read all green taxi trips from 2014 to 2015 from local directory, pass through a pipeline of data preparation steps and result in a final dataframe to be used in statistical modelling.  
```{r}
read_cols <- c("lpep_pickup_datetime", "Lpep_dropoff_datetime", "RateCodeID",
               "Pickup_longitude", "Pickup_latitude", 
               "Dropoff_longitude", "Dropoff_latitude",
               "Passenger_count", "Trip_distance", "Tip_amount", "Total_amount")

out_cols <- c("Season", "Pickup_month", "Pickup_day", "Weekend", "Pickup_hour", 
              "Time", "Pickup_zone", "Dropoff_zone", "Passenger_count",
              "Airport_trip", "Holiday", "AWND", "PRCP", "SNOW", "SNWD", "TAVG",
              "Trip_distance", "Trip_duration", "Great_circle_dist", "Total_fare")


read_one_taxi_data <- function(filename, read_columns=read_cols, out_columns=out_cols) {
  print(paste("Reading", filename))
  fread(filename, select=read_columns, fill=TRUE, showProgress=FALSE) %>%
    as_tibble() %>%
    compute_total_fare() %>%
    compute_trip_duration() %>%
    clean_taxi_data() %>%
    preprocess_taxi_data() %>%
    select(out_columns)
}

read_all_taxi_data <- function(path, pattern) {
  lapply(list.files(path, pattern, full.names=TRUE), 
         function(x) read_one_taxi_data(x, read_cols, out_cols)) %>%
    rbindlist()
}


dev <- FALSE

if (dev) {
  trips_data <- read_all_taxi_data("data", "green_tripdata")
  saveRDS(trips_data, file="green_trips_data_2014-15.rds")
}
```


The processed data has 34,095,643 rows and 20 columns and cost around 3.9~4 in RAM in local machine. In the fitting model section, we will turn to `h2o` package for its fast and scalable machine learning application.  


***


# Analysis


Before jumping into fitting statistical models, we may want to first analyse and report taxi fares (*response*) via descriptive statistics for a group of *explanatory* variables. In our case, we will do this on a random subsample of prepared dataset which contains 1 million records. A reasonable assumption here is that the records in this subset may very well be a fair representative sample of the entire dataset.  

```{r}
if (dev) {
  set.seed(100)
  n <- 1e6
  subsetted <- sample_n(trips_data, n)
  saveRDS(subsetted, file="1m_green_trips_data_2014-15.rds")
} else {
  subsetted <- readRDS("1m_green_trips_data_2014-15.rds")
}
```

## Trip distance and duration

In an attempt to infer result from *hypothesis 1*, we can see the effect of trip distance and duration on taxi fare by stratifying the observations into groups of different distance and duration. In Figure 1, we can see trips with long duration are generally associated with higher fare, whereas trip distance seems to have a smaller effect on the fare. Note that distance and duration were discretised into bins such that there are equal number of observations in each bin. A summary of mean total fare for each group is also provided below, as shown in Table 1.

```{r, fig.cap="**Figure 1**: Boxplot of taxi fares, stratified by groups of discretised trip distance and duration. (Cutoff at y=50 for visualization purpose)"}
# trip distance and trip duration vs total fare
n_bins <- 3
subsetted <- subsetted %>%
  mutate(Distance = cut_number(Trip_distance, n_bins),
         Duration = cut_number(Trip_duration, n_bins)) 

subsetted %>%
  ggplot(aes(x=Distance, y=Total_fare, fill=Duration)) +
  geom_boxplot() +
  ylim(c(0, 50)) +
  theme_minimal() +
  labs(title = "Trip distance and duration vs Total fare")

subsetted %>%
  group_by(Distance, Duration) %>%
  summarise(Mean_fare = mean(Total_fare)) %>%
  spread(Duration, Mean_fare) %>%
  rename("Distance / Duration" = Distance) %>%
  knitr::kable(digits=2, format="pandoc", 
               caption="Table 1: Mean total fare for each distance and duration group")
```


## Day of week and time

To get a sense of how different day and time affect the taxi fare (*hypothesis 4 and 5*), we may want to inspect the mean taxi fare across different days of week for both day and night trips. Note that day trips were defined as trips took place from 6:00 to 18:00 while night trips as trips from 18:00 to 6:00. From Figure 2, we can see that day trips and night trips have very different mean fare during the weekdays. This suggest that time of day may be a factor affecting the taxi fare depending on whether it is weekday or weekend. We can infer that night trips generally have lower taxi fare comparing to day trips during weekdays.  
```{r, fig.cap="**Figure 2**: Taxi fares of day trips and night trips across different days of week."}
# day of week and time of day
subsetted %>%
  group_by(Pickup_day, Time) %>%
  summarise(Total_fare = mean(Total_fare)) %>%
  ggplot(aes(x=Pickup_day, y=Total_fare, color=Time, group=Time)) + 
  geom_point(aes(pch=Time), size=2) +
  geom_line(aes(linetype=Time), size=1) +
  theme_minimal() +
  labs(title = "Pickup day and time vs (Mean) total fare")
```

## Effect of trip to/from airport

To understand the effect of airport rides on taxi fares (*hypothesis 6*), a plot of distribution of taxi fares for airport and non-airport trips is presented in Figure 3. As expected, taxi fares show very different distribution under both scenarios which could not be otherwise easily detected without grouping. For completeness, a summary of taxi fares is also provided in Table 2 which again shows that the distribution of fares differ between airport and non-airport trips.  
```{r, fig.cap="**Figure 3**: Density plot showing distribution of airport and non-airport taxi fares. (Cutoff at x=100 for visualization purpose)"}
# is it trip to/from airport
subsetted %>%
  ggplot(aes(x=Total_fare, fill=Airport_trip)) +
  geom_density(alpha=0.4) +
  xlim(c(0, 100)) +
  theme_minimal() +
  scale_fill_brewer(palette="Dark2") +
  labs(title = "Distribution of total fare for airport/non-airport trips")


subsetted %>%
  group_by(Airport_trip) %>%
  do(data_frame_(summary(.$Total_fare))) %>%
  knitr::kable(digits=2, format="pandoc", 
               caption="Table 2: A summary table of distribution of fares for airport vs non-airport rides")
```


## Correlation

Understanding the correlation between the explanatory variable may also be useful prior to building statistical models explaining relation of taxi fare with attributes. In this regard, a correlation matrix plot is presented in Figure 4. From the plot, we can see that total fare is weakly or uncorrelated with the different weather condition measures (*hypothesis 7*). This potentially arises due to using a crude and coarse-grained weather dataset. On the other hand, trip distance, duration and great circle distance computed seem to have strong positive correlation with the response. While circle distance is strongly correlated with trip distance, we do expect that either one of them and trip duration to be strong predictors for taxi fare.
```{r fig.width=7, fig.height=7, fig.cap="**Figure 4**: Correlation plot of numeric explanatory variables. Notice that number of passengers seems irrelevant to any other attributes and could simply be removed in downstream analysis or ignored."}
corr <- subsetted %>%
  select(-Pickup_hour) %>%
  select_if(is.numeric) %>%
  cor()

# save space
rm(subsetted)

corrplot(corr, type="lower", method="color", 
         addCoef.col="black", order="hclust",
         tl.srt = 45, tl.col="black", 
         number.cex=1, diag=FALSE, 
         title="Correlation of numeric variables",
         mar=c(0, 0, 1, 0))

```


***


# Fit model

We will now build statistical models, each with different underlying assumptions about the data to better explain the relation between taxi fare and other attributes. Due to time and physical limitations of RAM and disk storage, we will only build models and train on 3 millions of the data -- randomly sampled from the entire processed dataset. To do this, as mentioned previously we will use R's interface to `h2o` which handles large scale machine learning algorithm by leveraging computing power of distributed systems and ultilizing parallelized algorithms similar to in-memory mapreduce operations.  

Traning and validation sets each containing 3 millions of records are first created. 

```{r}
if (dev) {
  set.seed(100)
  n <- 3e6
  train_idx <- sample(nrow(trips_data), n)
  
  train <- trips_data[train_idx,] %>%
    mutate(Pickup_month = factor(Pickup_month, ordered=FALSE),
           Pickup_day = factor(Pickup_hour, ordered=FALSE))
  
  test <- trips_data[-train_idx,] %>%
    sample_n(n) %>%
    mutate(Pickup_month = factor(Pickup_month, ordered=FALSE),
           Pickup_day = factor(Pickup_hour, ordered=FALSE))
}
```


In the context of fitting statistical models, we will build 3 models with `Total_fare` as the *response* and the rest attributes as *predictors*:  

- **Multiple Regression (GLM)**: This is essentially linear regression with Gaussian family model with the identity link function. It models the dependency between the *response* and *predictors* as a linear function. It has the advantages of being fast and easy to interprete over other machine learning models.  
- **Distributed Random Forest (DRF)**: DRF generates a forest of regression trees, each of which is a weak learner built on a subset of observations and attributes. Regression with average prediction over all of the trees to make a final prediction usually results in a lower variance of the model.  
- **Gradient Boosting Machine (GBM)**: A forward learning ensemble method which builds regression trees on all the features in the dataset in a fully distributed way -- each tree is built in parallel. Briefly, weak regression algorithms are sequentially applied to the incrementally changed data to create a series of decision trees, producing an ensemble of weak prediction models. In general, boosting is a flexible nonlinear regression procedure that helps improve the
accuracy of trees<sup>[8](https://en.wikipedia.org/wiki/Gradient_boosting)</sup>.  

It should be noted that categorical encoding is carried out while fitting the GLM. The values of all numeric predictors are also standarized to have zero mean and unit variance for ease of comparison downstream.  

```{r}

capture.output(localH2O <- h2o.init(nthreads = -1), file="/dev/null")

if (dev) {
  # save space
  rm(trips_data)
  
  train.h2o <- as.h2o(train)
  rm(train)
  test.h2o <- as.h2o(test)
  rm(test)
  
  response <- "Total_fare"
  predictors <- setdiff(names(train.h2o), response)
  
  regression.model <- h2o.glm(y=response, x=predictors, training_frame=train.h2o, family="gaussian",
                               validation_frame=test.h2o, standardize=TRUE, 
                               lambda_search=FALSE, alpha=0, lambda=0)
  
  glm_path <- h2o.saveModel(regression.model, path="models/", force=TRUE)
  
  print(paste("GLM saved in", glm_path))
  
  
  
  
  rforest.model <- h2o.randomForest(y=response, x=predictors, training_frame=train.h2o, 
                                    validation_frame=test.h2o,
                                    ntrees=1000, mtries=3, max_depth=4, seed=100)
  
  
  rf_path <- h2o.saveModel(rforest.model, path="models/", force=TRUE)
  
  print(paste("RForest saved in", rf_path))
  
  
  
  
  gbm.model <- h2o.gbm(y=response, x=predictors, training_frame=train.h2o, 
                       validation_frame=test.h2o,
                       ntrees=1000, max_depth=4, learn_rate=0.01, seed=100)
  
  
  gbm_path <- h2o.saveModel(gbm.model, path="models/", force=TRUE)
  
  print(paste("GBM saved in", gbm_path))
  
} else {
  # load saved model
  regression.model <- h2o.loadModel("models/GLM_model_R_1567913957322_10")
  rforest.model <- h2o.loadModel("models/DRF_model_R_1567913957322_2")
  gbm.model <- h2o.loadModel("models/GBM_model_R_1567913957322_3")
}
```



***

# Results

It is important to interpret the models and understand how they have performed without treating them as black boxes. In this section we will examine how each of them performed on training and validation sets.  


## Comparing models


To understand how the models performed, barcharts showing performance metrics of each model on training and validation sets are presented in Figure 5. From the figure, surprisingly DRF performed poorly comparing to other two models. This could arise from the fact that most predictors are not useful in predicting the taxi fare -- thus ensemble of weak learners actually have high bias.   

Nevertheless, we managed to see GLM and GBM performed well in terms of root mean squared error (RMSE) and R squared (R2). In particular, despite of longer training time, GBM achieved an astonishing 2.21 for RMSE and 0.941 for R2 -- which in turn translates on average 2.21$ difference between actual fare and predicted fare amount and 94.1% of the actual taxi fare's variation explained by the model. Table 3 shows summary of performance metrics of each models in tabular form.  

```{r, fig.cap="**Figure 5**: Performance metrics of all 3 models on training and validation sets."}
extract_metrics <- function(...) {
  lapply(..., function(model) {
    data.frame(
      list(rmse=h2o.rmse(model, train=TRUE, valid=TRUE),
           r_squared=h2o.r2(model, train=TRUE, valid=TRUE), 
           mean_residual_deviance=h2o.mean_residual_deviance(model, train=TRUE, valid=TRUE),
           training_time=c("train"=model@model$run_time / 1000, "valid"=NA))) %>%
      rownames_to_column("Dataset") %>%
      gather(Measure, Value, -Dataset) %>%
      mutate(Model = model@algorithm) %>%
      drop_na()
  }) %>%
    bind_rows()
}


extract_metrics(c(regression.model, rforest.model, gbm.model)) %>%
  mutate(Model = factor(Model, levels=c("glm", "drf", "gbm"))) %>%
  ggplot(aes(x=Model, y=Value, fill=Dataset)) +
  geom_bar(stat="identity", position=position_dodge(), color="black") +
  facet_wrap(Measure ~ ., scales="free_y") +
  scale_fill_brewer(palette="Set2") +
  labs(title="Performance of models") +
  theme_bw()

performance_table <- function(...) {
  extract_metrics(...) %>%
    filter(Measure == "rmse" | Measure == "r_squared" | Measure == "training_time") %>%
    unite("Dataset_Measure", c("Dataset", "Measure")) %>%
    spread(Dataset_Measure, Value) %>%
    select(Model, Train_RMSE=train_rmse, Validation_RMSE=valid_rmse,
           Train_R2=train_r_squared, Validation_R2=valid_r_squared,
           Training_time=train_training_time)
}

performance_table(c(regression.model, rforest.model, gbm.model)) %>%
  mutate(Model = toupper(Model),
         Model = factor(Model, levels=c("GLM", "DRF", "GBM"))) %>%
  arrange(Model) %>%
  knitr::kable(digits=3, format="pandoc", 
               caption="Table 3: Performance metrics of all 3 models in tabular form.")

```


## Variable importances

In an attempt to understand how predictors contributed in the predicting the taxi fare, we will use different measures for each model in order to rank the importance of each predictors.  

In the case of GLM, we will compare the standardized coefficients which represent the mean change in the response given a one standard deviation change in each predictor. In general, a predictor with high magnitude of standardized coefficient indicates that it is important in predicting the response. From Figure 6, we see top 20 predictors with the highest standardized coefficients. As expected, trip distance contributes a lot in predicting the taxi fare, followed by other location and airport attributes (*hypothesis 1, 3 and 6*).  

```{r, fig.cap="**Figure 6**: Barplot showing standardized coefficient magnitudes of top 20 predictors."}
## plot bar chart representing the standardized coefficient magnitude
list("std_coef"=h2o.coef_norm(regression.model)) %>%
  data.frame() %>%
  rownames_to_column("variable") %>%
  mutate(abs = abs(std_coef),
         sign = factor(ifelse(std_coef > 0, "Positive", "Negative"))) %>%
  arrange(-abs) %>%
  head(20) %>%
  ggplot(aes(x=reorder(variable, abs), y=abs, fill=sign)) +
  geom_bar(stat="identity", color="black") +
  coord_flip() +
  labs(title="Standardized coefficient magnitudes of \nGLM",
       x="Magnitude",
       y="Variable") +
  theme_minimal()
```

On the other hand, for the tree-based models, we can determine the variable importance by calculating the relative influence of each variable, that is, whether that variable was selected to split on during the tree building process, and how much the RMSE over all trees improved as a result. Specifically, Figure 7 shows the top 10 predictors with highest variable importance in estimating taxi fare for the GBM. Again it suggests that trip distance could potentially be the most important feature in determining the taxi fare, followed by trip duration. Interestingly, other predictors seemed to contribute little to predicting taxi fare.  

```{r, fig.cap="**Figure 7**: Variable importance of each predictor in GBM."}
# plot graph of the variable importances
h2o.varimp(gbm.model) %>%
  as_tibble() %>%
  select(variable, percentage) %>%
  arrange(-percentage) %>%
  head(10) %>%
  ggplot(aes(x=reorder(variable, percentage), y=percentage, fill=percentage)) +
  geom_bar(stat="identity", color="black") +
  coord_flip() +
  labs(title="Percentage of feature importances of GBM",
       x="Percentage",
       y="Variable") +
  theme_minimal()
```



***

# Refine model


We will now improve GLM and GBM by performing the following steps:  

- Remove insignificant predictors such as weather condition and passenger count.  
- Fit GLM with elastic net regularization<sup>[9](https://en.wikipedia.org/wiki/Elastic_net_regularization)]</sup> and search for best lambda (regularization parameter).  
- Fit GBM with more and deeper trees.  

A summary of performance of improved models is shown in Table 4.  

```{r}
if (dev) {
  elastic_net.model <- h2o.glm(y=response, x=predictors, training_frame=train.h2o, family="gaussian",
                               validation_frame=test.h2o, standardize=TRUE, 
                               lambda_search=TRUE)
  
  elastic_net_path <- h2o.saveModel(elastic_net.model, path="models/", force=TRUE)
  
  print(paste("Elastic net regression saved in", elastic_net_path))

  deepgbm.model <- h2o.gbm(y=response, x=predictors, training_frame=train.h2o, 
                           validation_frame=test.h2o,
                           ntrees=2000, max_depth=6, learn_rate=0.05, seed=100)
  
  deepgbm_path <- h2o.saveModel(deepgbm.model, path="models/", force=TRUE)
  
  print(paste("Deep GBM saved in", deepgbm_path))
} else {
  elastic_net.model <- h2o.loadModel("models/GLM_model_R_1567913957322_9")
  deepgbm.model <- h2o.loadModel("models/GBM_model_R_1567913957322_11")
}

performance_table(c(elastic_net.model, deepgbm.model)) %>%
  mutate(Model = ifelse(Model == "glm", "Elastic_net", "Deep_GBM"),
         Model = factor(Model, levels=c("Elastic_net", "Deep_GBM"))) %>%
  arrange(Model) %>%
  knitr::kable(digits=3, format="pandoc", 
               caption="Table 4: Summary of improved models.")
```

## Final model




## Further considerations

```{r}
if (dev) {
  nestedgbm.model <- h2o.gbm(y=response, x=setdiff(predictors, c("Trip_distance", "Trip_duration")),
                             training_frame=train.h2o, validation_frame=test.h2o,
                             ntrees=2000, max_depth=6, learn_rate=0.05, seed=100)
  
  nestedgbm_path <- h2o.saveModel(a.model, path="models/", force=TRUE)
  
  print(paste("A saved in", nestedgbm_path))
} else {
  nestedgbm.model <- h2o.loadModel("models/GBM_model_R_1567913957322_12")
}

performance_table(list(nestedgbm.model)) %>%
  mutate(Model = "Nested_GBM") %>%
  knitr::kable(digits=3, format="pandoc", caption="Table 6")
```





```{r}
h2o.removeAll()
capture.output(h2o.shutdown(prompt=FALSE), file="/dev/null")
```


***

# Conclusion
